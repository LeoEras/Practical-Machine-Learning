---
title: "Practical Machine Learning, final Project"
author: "Leonardo Eras"
date: "9/23/2020"
output:
  pdf_document: default
  html_document: default
---

## Summary
Executive Summary Human Activity Recognition (HAR) is a key research area that is gaining increasing attention, especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises. Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.

Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

## Data Exploration
First we'll load some useful libraries and the datasets from the web as URLs (This can take a while, so be patient).
```{r}
library(ggplot2)
library(caret)
library(knitr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)

training<-read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testing<-read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

dim(testing)
dim(training)
```
We can build a training and validation sets from the training variable. We'll also set our seed here before proceeding.
```{r}
set.seed(12321)
index<-createDataPartition(training$classe, p=0.7, list=FALSE)
trainSet<-training[index, ]
validSet<-training[-index, ]
```

The 20 cases the quiz wants us to predict are stored in the testing$problem_id variable. This is what we're going to predict. Let's clean the training dataset form NAs!

```{r}
NZV <- nearZeroVar(trainSet) #Check documentation with ?nearZeroVar
trainSet <- trainSet[, -NZV]
validSet  <- validSet[, -NZV]

nas <- sapply(trainSet, function(x) mean(is.na(x))) > 0.95
trainSet <- trainSet[, nas==FALSE]
validSet  <- validSet[, nas==FALSE]

trainSet <- trainSet[, -(1:5)]
validSet  <- validSet[, -(1:5)]
```

I'll try some models on the sets, the one with higher accuracy will be the one to use for the testing set.
```{r}
#If this is not set, it might take a looooooong time to run
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
rf_model<-train(classe~., method="rf", data=trainSet, trControl=controlRF)
```
Ok let's check the accuracy of this model
```{r}
rf_model$finalModel
```
It does looks promissing! Let's check how does it perform on the validation set.

```{r}
prf <- predict(rf_model, newdata=validSet)
cmrf <- confusionMatrix(prf, validSet$classe)
cmrf
```

Let's see how does it perform on the testing set.
```{r}
finalrfprediction <- predict(rf_model, newdata=testing)
finalrfprediction
```

Awesome! We might stop with this and call it a day, but I'm going to test some other methods.

## Generalized Boosting Model
```{r}
controlgbm <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
gbm_model  <- train(classe ~ ., data=trainSet, method = "gbm",
                    trControl = controlgbm, verbose = FALSE)
```

Let's see what we have with this model
```{r}
pgbm <- predict(gbm_model, newdata=validSet)
cmgbm <- confusionMatrix(pgbm, validSet$classe)
cmgbm
```
This one seems good to, just not as good as a random forest! Let's see how does it perform on the testing set.
```{r}
finalgbmprediction <- predict(gbm_model, newdata=testing)
finalgbmprediction
```

## Linear discriminant analysis
```{r}
controllda <- trainControl(method = "cv", number = 3)
lda_model  <- train(classe ~ ., data=trainSet, method = "lda",
                    trControl = controllda, verbose = FALSE)
```

Let's see what where can we go with this.
```{r}
plda <- predict(lda_model, newdata=validSet)
cmlda <- confusionMatrix(plda, validSet$classe)
cmlda
```

Ok, this one has the least accuracy, but it's over 0.5, so it's not random :D.  Let's see how does it perform on the testing set.
```{r}
finalldaprediction <- predict(lda_model, newdata=testing)
finalldaprediction
```
